{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Mar 28 14:04:56 2021\n",
    "@author: Saptarshi mukhopadhaya / Salim Hafid\n",
    "\"\"\"\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import senti_bignomics\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import pysentiment2 as ps\n",
    "from nltk import sent_tokenize\n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, make_scorer, accuracy_score, f1_score, precision_score, recall_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "#warnings.suppress(label_encoder_deprecation_msg, UserWarning)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import spacy\n",
    "import re\n",
    "import math\n",
    "import operator\n",
    "import constants\n",
    "from sklearn.model_selection import cross_validate, RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(y_test,y_pred_df):\n",
    "    plt.figure(figsize=(12,5));\n",
    "    plt.title('Comparzison actual vs predicted')\n",
    "    plt.plot(y_test)\n",
    "    y_test.plot(legend=True) \n",
    "    plt.plot(y_pred_df)\n",
    "    plt.show()\n",
    "    \n",
    "def correlation_raph(frame):\n",
    "    frame['change'] = frame['close_days'] -frame['close']\n",
    "    frame = frame.tail(100)\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "    \n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('time')\n",
    "    ax1.set_ylabel('polarity score', color=color)\n",
    "    ax1.plot(frame.index, frame['rel_pol'], color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "    \n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('change in $', color=color)  # we already handled the x-label with ax1\n",
    "    ax2.plot(frame.index, frame['change'], color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_window_news():\n",
    "    pass\n",
    "\n",
    "def avg_news(df,pop):\n",
    "    period = len(df)\n",
    "    total_news = 0\n",
    "    for i in df['text']:\n",
    "        if not isinstance(i, float):\n",
    "            total_news = total_news+len(i)\n",
    "    buckets = period/pop\n",
    "    return total_news/buckets\n",
    "    \n",
    "def prepare_data(path,column):\n",
    "    path = path.sort_values(by=['time'])\n",
    "    path = path.mask(path.eq('None')).dropna()\n",
    "    path = path.drop_duplicates()\n",
    "    path['date'] = pd.to_datetime(path['time']).dt.date\n",
    "    path['time'] = pd.to_datetime(path['time']).dt.time\n",
    "    path['date'] = pd.to_datetime(path['date'])\n",
    "    path = path[['time','date',column]]\n",
    "    return(path)\n",
    "\n",
    "\n",
    "def load_text_data(path,column):\n",
    "    df = sort_news_by_time(prepare_data(pd.read_csv(path),column),column)\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    #print(df.dtypes)\n",
    "    return df\n",
    "\n",
    "def merge_frame(df,data,col):\n",
    "    data = data.set_index('time')\n",
    "    frame = df.join(data,on=col,how = 'left')\n",
    "    #frame['pol'] = frame['pol'].fillna (0)\n",
    "    return frame\n",
    "\n",
    "def load_price_data(path):\n",
    "    data = pd.read_csv(path)\n",
    "    data = data[['time','open','close']]\n",
    "    data = data.sort_values(by=['time'])\n",
    "    #data = data.where(data['time']>'2019-01-01')\n",
    "    data['time'] = pd.to_datetime(data['time'])\n",
    "    #data = data.set_index('time')\n",
    "    #print(data.dtypes)\n",
    "    data = data.dropna()\n",
    "    return data\n",
    "\n",
    "\n",
    "'''Sort the news by time and date'''\n",
    "def sort_news_by_time(frame,col):\n",
    "    company = {}\n",
    "    new_frame = {}\n",
    "    words = None\n",
    "    prev = None\n",
    "    for i in frame.values:\n",
    "        #print(i)\n",
    "        time,key,val=str(i[0]),str(i[1]),i[2]\n",
    "        #print(i)\n",
    "        if (key>='2018-12-29' and key<='2020-10-01'):\n",
    "            if prev == key and time<'21:00:00':\n",
    "                words.append(val)\n",
    "            else:\n",
    "                if prev is not None:\n",
    "                    company[prev] = words\n",
    "                words = []\n",
    "                prev = key\n",
    "                words.append(val)\n",
    "        company[prev] = words\n",
    "    #print(company.keys())\n",
    "    \n",
    "    new_frame['time'] = company.keys()\n",
    "    new_frame['text'] = company.values()\n",
    "    return pd.DataFrame.from_dict(new_frame).iloc[1:,:]\n",
    "\n",
    "def cleaning(sentences):\n",
    "    nlp = spacy.load('en_core_web_sm',disable=['ner','parser'])\n",
    "    company_stock_names = ['fb', 'facebook','googl','google', 'amzn', 'amazon', 'aapl', 'apple', 'amgn', 'amgen', 'aligntechnology', 'msft', 'microsoft','nflx','netflix','aal','americanairlines']\n",
    "    day_names = ['monday','tuesday','wednesday','friday','saturday','sunday']\n",
    "    month_names = ['january','february','march','april','may','june','july','august','september','october','november','december']\n",
    "    time_period_names = ['today','tomorrow','yesterday','day','week','month','year','daily','weekly','monthly','yearly']\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        # Lemmatizing + Removing stopwords + Removing non-alphabetical characters + Removing company stock names\n",
    "        # + Removing useless frequent words (day names, time period names, company stock names)\n",
    "        tokens = [token.lemma_ for token in doc if\n",
    "                  not token.is_stop \n",
    "                  and str(token) not in company_stock_names\n",
    "                  and str(token) not in day_names\n",
    "                  and str(token) not in month_names\n",
    "                  and str(token) not in time_period_names\n",
    "                 ]\n",
    "        # Remove non-alphabetic characters\n",
    "        tokens = [re.sub(\"[^A-Za-z']+\", '', str(token)).lower() for token in tokens]\n",
    "        tokens = [token for token in tokens if len(token)>1]\n",
    "        # Since Word2Vec uses surrounding words in a sentence, a sentence that contains less than 3 tokens is not useful\n",
    "        if len(tokens) > 2:\n",
    "            cleaned_sentences.append(tokens)\n",
    "        else:\n",
    "            cleaned_sentences.append(['blank_to_keep_the_index_coherent_with_the_sentiment_feature'])            \n",
    "    return cleaned_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Calculate sentiment with weighted approch using vader'''  \n",
    "def weight_sentiment_scores(news): \n",
    "    count = 0\n",
    "    polarity = 0\n",
    "    # Create a SentimentIntensityAnalyzer object. \n",
    "    sid_obj = SentimentIntensityAnalyzer() \n",
    "    weight = len(news)\n",
    "    # polarity_scores method of SentimentIntensityAnalyzer \n",
    "    # oject gives a sentiment dictionary. \n",
    "    # which contains pos, neg, neu, and compound scores. \n",
    "    for i in range(len(news)):\n",
    "        \n",
    "        #print(i)\n",
    "        if not isinstance(news[i],float):\n",
    "            weight = weight - i\n",
    "            sens = sent_tokenize(news[i])\n",
    "            weight = weight - i\n",
    "            \n",
    "            #for j in sens:\n",
    "                #if comp.lower() in j.lower():\n",
    "            sentiment_dict = sid_obj.polarity_scores(news[i]) \n",
    "            polarity = polarity+(sentiment_dict['compound']*weight)\n",
    "            count = count+weight\n",
    "            \n",
    "    if count:\n",
    "        return polarity/count\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "'''Calculate sentiment with weighted approch using textblob'''    \n",
    "def weight_sentiment_textblob(news):\n",
    "    count = 0\n",
    "    polarity = 0\n",
    "    weight = len(news)\n",
    "    for i in news:\n",
    "        sen = TextBlob(i)\n",
    "        if sen.sentiment.polarity != 0.0:\n",
    "            polarity = polarity+(sen.sentiment.polarity*weight)\n",
    "            count = count+weight\n",
    "    if count:\n",
    "        return polarity/count\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "'''Calculate sentiment with weighted approch using pysentiment'''   \n",
    "def weight_sentiment_pysentiment(news):\n",
    "    lm = ps.LM()\n",
    "    count = 0\n",
    "    polarity = 0\n",
    "    weight = len(news)\n",
    "    for i in news:\n",
    "        #print(i)\n",
    "        tokens = lm.tokenize(i)\n",
    "        score = lm.get_score(tokens)\n",
    "        if score['Subjectivity']!= 0.0:\n",
    "            #print(score['Polarity'])\n",
    "            polarity = polarity+score['Polarity']*weight\n",
    "            count = count+weight\n",
    "    if count:\n",
    "        return polarity/count\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "'''update the vader lexicon with news corpus'''\n",
    "def update_vader(vader,corpus):\n",
    "    return vader.lexicon.update(corpus)\n",
    "\n",
    "sid_obj = SentimentIntensityAnalyzer() \n",
    "\n",
    "'''Scale the scores of the words in Sentibignomics'''\n",
    "for i in senti_bignomics.senti_bignomics:\n",
    "    if not type(senti_bignomics.senti_bignomics[i]) == float:\n",
    "        senti_bignomics.senti_bignomics[i] = 4*float(senti_bignomics.senti_bignomics[i][0])\n",
    "update_vader(sid_obj,senti_bignomics.senti_bignomics)    \n",
    "\n",
    "'''Calculate the error'''\n",
    "def get_error(y_test,y_pred):\n",
    "    return np.sqrt(np.mean(np.square(((y_test - y_pred) / y_test)), axis=0))*100\n",
    "\n",
    "\n",
    "'''Calculate sentiment using window'''    \n",
    "def window_sentiment(data,window):\n",
    "    news_list = []\n",
    "    news = []\n",
    "     \n",
    "    data['news'] = data['pol']\n",
    "    for i in range(0,window+1):\n",
    "        #print(data['news'].shift(i).fillna('[]'))\n",
    "        data['news'] = data['news']+data['pol'].shift(i)\n",
    "    \n",
    "    #print(len(news_list))\n",
    "    return data['news']/(window)  \n",
    "\n",
    "#def update_vader(vader,corpus):\n",
    " #   return vader.lexicon.update(corpus)\n",
    "def change_sentiment(frame,window):\n",
    "    change = []\n",
    "    frame['a']=frame['close'].shift(window)-frame['close']\n",
    "    #print(frame)\n",
    "    for i in frame['a']:\n",
    "        if i >= 0:\n",
    "            change.append(1)\n",
    "        else:\n",
    "            change.append(0)\n",
    "        '''\n",
    "        if i > 0:\n",
    "            change.append(1)\n",
    "        elif i<0:\n",
    "            change.append(-1)\n",
    "        else:\n",
    "            change.append(0) \n",
    "        '''\n",
    "    frame['change'] = change\n",
    "    #print(frame)\n",
    "    return frame\n",
    "\n",
    "def get_sentiment(frame):\n",
    "    pol_list = []\n",
    "    for i in frame['text']:\n",
    "        if not isinstance(i,float):\n",
    "            pol_list.append(weight_sentiment_scores(i))\n",
    "        else:\n",
    "            pol_list.append(0)\n",
    "    frame['pol'] = pol_list\n",
    "    return frame\n",
    "\n",
    "'''Calculate sentiment using vader without weight'''\n",
    "def sentiment_scores(news): \n",
    "    count = 0\n",
    "    polarity = 0\n",
    "    # Create a SentimentIntensityAnalyzer object. \n",
    "    \n",
    "    \n",
    "    for i in news:\n",
    "        \n",
    "        if not isinstance(i,float):\n",
    "            \n",
    "            sentiment_dict = sid_obj.polarity_scores(i) \n",
    "            polarity = polarity+(sentiment_dict['compound'])\n",
    "            count = count+1\n",
    "            \n",
    "    if count:\n",
    "        return polarity/count\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count dictionary\n",
    "word_freq = {}\n",
    "for sent in cleaned_articles:\n",
    "    for i in sent:\n",
    "        if i in word_freq.keys():\n",
    "            word_freq[i] += 1\n",
    "        else:\n",
    "            word_freq[i] = 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent words\n",
    "sorted(word_freq, key=word_freq.get, reverse=True)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event-extraction\n",
    "\n",
    "def create_event_vector(data):\n",
    "    event_vector = {}\n",
    "    for idx,article in enumerate(data):\n",
    "        event_vector[idx] = {}\n",
    "        for event in constants.FINANCIAL_EVENTS:\n",
    "            event_vector[idx][event] = 0\n",
    "        for token in article:\n",
    "            for event in constants.FINANCIAL_EVENTS:\n",
    "                if token in constants.FINANCIAL_EVENTS[event]:\n",
    "                    if event in event_vector[idx]:\n",
    "                        event_vector[idx][event] += 1\n",
    "                    else:\n",
    "                        event_vector[idx][event] = 1\n",
    "    return event_vector\n",
    "                \n",
    "\n",
    "def convert_event_vector(event_vector):\n",
    "    for idx in event_vector:\n",
    "        for event in event_vector[idx]:\n",
    "            if event_vector[idx][event] != 0:\n",
    "                event_vector[idx][event] = math.log(event_vector[idx][event])\n",
    "            else:\n",
    "                event_vector[idx][event] = -999\n",
    "    return event_vector\n",
    "\n",
    "def get_predicted_event(event_vector):\n",
    "    predicted_events = []\n",
    "    for article in event_vector:\n",
    "        max_val = max(event_vector[article].items(), key=operator.itemgetter(1))[1]\n",
    "        if max_val > 0:\n",
    "            key_with_max_val = max(event_vector[article].items(), key=operator.itemgetter(1))[0]\n",
    "            predicted_events.append(key_with_max_val)\n",
    "        else:\n",
    "            predicted_events.append('none')\n",
    "    return predicted_events\n",
    "\n",
    "def get_weighted_scores(predicted_events):\n",
    "    weighted_scores = []\n",
    "    for event in predicted_events:\n",
    "        if event != 'none':\n",
    "            weighted_scores.append(constants.FINANCIAL_EVENTS_WEIGHTS[event])\n",
    "        else:\n",
    "            weighted_scores.append(1)\n",
    "    return weighted_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_model(df):\n",
    "    model = RandomForestClassifier()\n",
    "    df = df.drop(['text'], axis=1)\n",
    "    \n",
    "    labels = df.change\n",
    "    \n",
    "    #print(labels)\n",
    "    df = df.drop(['change'], axis=1)\n",
    "    df = df[['close','rel_pol']]\n",
    "    X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(df, labels, df.index, test_size=0.30, random_state=0,shuffle=False)\n",
    "    print(y_train.value_counts(normalize = False))\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print('Classification report:\\n ',classification_report(y_test,y_pred))\n",
    "    print('confusion_matrix:\\n',confusion_matrix(y_test,y_pred))\n",
    "    #print(pd.crosstab(y_test,y_pred))\n",
    "\n",
    "\n",
    "'''Get the prediction'''    \n",
    "def get_prediction(data,no_news):\n",
    "    data_new = data\n",
    "    if no_news:\n",
    "        \"Specify input and output variable\"\n",
    "        x_data = data_new.drop(['close_days','rel_pol'], axis=1)\n",
    "        y_data = data_new['close_days']\n",
    "    else:\n",
    "        x_data = data_new.drop(['close_days'], axis=1)\n",
    "        y_data = data_new['close_days']\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3,shuffle=False)\n",
    "    \n",
    "    lr_model=LinearRegression()\n",
    "    \n",
    "    lr_model.fit(x_train,y_train)\n",
    "    \n",
    "    y_pred=lr_model.predict(x_test)\n",
    "    y_pred_df = pd.DataFrame(y_pred, index= y_test.index)\n",
    "    #print(x_data.columns)\n",
    "    plot_graph(y_test,y_pred_df)\n",
    "    print('r2-score:',r2_score(y_test,y_pred))\n",
    "    print('mean_squared_error:',mean_squared_error(y_test,y_pred))\n",
    "    return get_error(y_test,y_pred)\n",
    "\n",
    "def nested_classification_report(y_true,y_pred):\n",
    "    print('Classification report:\\n ',classification_report(y_true,y_pred))\n",
    "    print('confusion_matrix:\\n',confusion_matrix(y_true,y_pred))\n",
    "    return accuracy_score(y_true,y_pred)\n",
    "\n",
    "def optimized_classifier(df):\n",
    "    model = RandomForestClassifier()\n",
    "    df = df.drop(['text'], axis=1)\n",
    "    labels = df.change\n",
    "    df = df.drop(['change'], axis=1)\n",
    "    #df = df[['close','rel_pol']]\n",
    "    #df = df[['close']]\n",
    "    df = df[['rel_pol']]\n",
    "    \n",
    "    tuning_grid = {\n",
    "        'max_features': [\"sqrt\", \"log2\"],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    # RF Model\n",
    "    random_forest_tuned = RandomizedSearchCV(model, tuning_grid, n_iter=100, scoring=\"f1_micro\", n_jobs=-1, cv=5, verbose=2, random_state=42)\n",
    "    \n",
    "    # Cross-validation version : \n",
    "    #scores = cross_validate(best_random_forest, df, labels, cv=5, scoring=[\"f1_micro\"], return_train_score=True)\n",
    "    #scores = cross_validate(best_random_forest, df, labels, cv=5, scoring=make_scorer(nested_classification_report))\n",
    "    #print(scores)\n",
    "    \n",
    "    # Single-validation version\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df, labels, test_size=0.3,shuffle=False)\n",
    "    _ = random_forest_tuned.fit(x_train, y_train)\n",
    "    best_random_forest = random_forest_tuned.best_estimator_\n",
    "    \n",
    "    \n",
    "    #print('x_test :\\n',x_test)\n",
    "    #print('y_test :\\n',y_test)\n",
    "    #print('y_pred :\\n',best_random_forest.predict(x_test))\n",
    "    \n",
    "    print('Classification report:\\n ',classification_report(y_test,best_random_forest.predict(x_test)))\n",
    "    \n",
    "    print('f1-micro :',f1_score(y_test,best_random_forest.predict(x_test),average='micro'))\n",
    "    print('precision :',precision_score(y_test,best_random_forest.predict(x_test),average='micro'))\n",
    "    print('recall :',recall_score(y_test,best_random_forest.predict(x_test),average='micro'))\n",
    "    \n",
    "    print('confusion_matrix:\\n',confusion_matrix(y_test,best_random_forest.predict(x_test)))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test,best_random_forest.predict(x_test)).ravel()\n",
    "    print('TN={},FP={},FN={},TP={}'.format(tn,fp,fn,tp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoP : 2\n",
      "Avg news count within the pop: 110.86935866983373\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/salim/.local/lib/python3.6/site-packages/sklearn/model_selection/_search.py:282: UserWarning: The total space of parameters 4 is smaller than n_iter=100. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  20 | elapsed:    0.3s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.84      0.66        69\n",
      "           1       0.39      0.12      0.19        57\n",
      "\n",
      "    accuracy                           0.52       126\n",
      "   macro avg       0.46      0.48      0.42       126\n",
      "weighted avg       0.47      0.52      0.44       126\n",
      "\n",
      "f1-micro : 0.5158730158730159\n",
      "precision : 0.5158730158730159\n",
      "recall : 0.5158730158730159\n",
      "confusion_matrix:\n",
      " [[58 11]\n",
      " [50  7]]\n",
      "TN=58,FP=11,FN=50,TP=7\n",
      "-------------------------\n",
      "PoP : 7\n",
      "Avg news count within the pop: 388.042755344418\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/salim/.local/lib/python3.6/site-packages/sklearn/model_selection/_search.py:282: UserWarning: The total space of parameters 4 is smaller than n_iter=100. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  20 | elapsed:    0.3s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.65        69\n",
      "           1       0.51      0.33      0.40        57\n",
      "\n",
      "    accuracy                           0.56       126\n",
      "   macro avg       0.54      0.54      0.52       126\n",
      "weighted avg       0.55      0.56      0.54       126\n",
      "\n",
      "f1-micro : 0.5555555555555556\n",
      "precision : 0.5555555555555556\n",
      "recall : 0.5555555555555556\n",
      "confusion_matrix:\n",
      " [[51 18]\n",
      " [38 19]]\n",
      "TN=51,FP=18,FN=38,TP=19\n",
      "-------------------------\n",
      "PoP : 12\n",
      "Avg news count within the pop: 665.2161520190024\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/salim/.local/lib/python3.6/site-packages/sklearn/model_selection/_search.py:282: UserWarning: The total space of parameters 4 is smaller than n_iter=100. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  20 | elapsed:    0.3s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.64      0.58        69\n",
      "           1       0.40      0.30      0.34        57\n",
      "\n",
      "    accuracy                           0.48       126\n",
      "   macro avg       0.46      0.47      0.46       126\n",
      "weighted avg       0.47      0.48      0.47       126\n",
      "\n",
      "f1-micro : 0.48412698412698413\n",
      "precision : 0.48412698412698413\n",
      "recall : 0.48412698412698413\n",
      "confusion_matrix:\n",
      " [[44 25]\n",
      " [40 17]]\n",
      "TN=44,FP=25,FN=40,TP=17\n",
      "-------------------------\n",
      "PoP : 17\n",
      "Avg news count within the pop: 942.3895486935867\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/salim/.local/lib/python3.6/site-packages/sklearn/model_selection/_search.py:282: UserWarning: The total space of parameters 4 is smaller than n_iter=100. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  20 | elapsed:    0.3s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.57      0.52        69\n",
      "           1       0.35      0.28      0.31        57\n",
      "\n",
      "    accuracy                           0.44       126\n",
      "   macro avg       0.42      0.42      0.42       126\n",
      "weighted avg       0.42      0.44      0.43       126\n",
      "\n",
      "f1-micro : 0.43650793650793657\n",
      "precision : 0.4365079365079365\n",
      "recall : 0.4365079365079365\n",
      "confusion_matrix:\n",
      " [[39 30]\n",
      " [41 16]]\n",
      "TN=39,FP=30,FN=41,TP=16\n",
      "-------------------------\n",
      "PoP : 22\n",
      "Avg news count within the pop: 1219.562945368171\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/salim/.local/lib/python3.6/site-packages/sklearn/model_selection/_search.py:282: UserWarning: The total space of parameters 4 is smaller than n_iter=100. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  20 | elapsed:    0.3s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.74      0.61        69\n",
      "           1       0.38      0.19      0.26        57\n",
      "\n",
      "    accuracy                           0.49       126\n",
      "   macro avg       0.45      0.47      0.44       126\n",
      "weighted avg       0.46      0.49      0.45       126\n",
      "\n",
      "f1-micro : 0.49206349206349204\n",
      "precision : 0.49206349206349204\n",
      "recall : 0.49206349206349204\n",
      "confusion_matrix:\n",
      " [[51 18]\n",
      " [46 11]]\n",
      "TN=51,FP=18,FN=46,TP=11\n",
      "-------------------------\n",
      "PoP : 27\n",
      "Avg news count within the pop: 1496.7363420427553\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/salim/.local/lib/python3.6/site-packages/sklearn/model_selection/_search.py:282: UserWarning: The total space of parameters 4 is smaller than n_iter=100. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  20 | elapsed:    0.3s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.71      0.61        68\n",
      "           1       0.44      0.28      0.34        57\n",
      "\n",
      "    accuracy                           0.51       125\n",
      "   macro avg       0.49      0.49      0.48       125\n",
      "weighted avg       0.50      0.51      0.49       125\n",
      "\n",
      "f1-micro : 0.512\n",
      "precision : 0.512\n",
      "recall : 0.512\n",
      "confusion_matrix:\n",
      " [[48 20]\n",
      " [41 16]]\n",
      "TN=48,FP=20,FN=41,TP=16\n",
      "-------------------------\n",
      "PoP : 32\n",
      "Avg news count within the pop: 1773.9097387173397\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/salim/.local/lib/python3.6/site-packages/sklearn/model_selection/_search.py:282: UserWarning: The total space of parameters 4 is smaller than n_iter=100. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  20 | elapsed:    0.3s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.84      0.68        68\n",
      "           1       0.52      0.22      0.31        55\n",
      "\n",
      "    accuracy                           0.56       123\n",
      "   macro avg       0.55      0.53      0.49       123\n",
      "weighted avg       0.55      0.56      0.51       123\n",
      "\n",
      "f1-micro : 0.5609756097560976\n",
      "precision : 0.5609756097560976\n",
      "recall : 0.5609756097560976\n",
      "confusion_matrix:\n",
      " [[57 11]\n",
      " [43 12]]\n",
      "TN=57,FP=11,FN=43,TP=12\n",
      "-------------------------\n",
      "PoP : 37\n",
      "Avg news count within the pop: 2051.083135391924\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/salim/.local/lib/python3.6/site-packages/sklearn/model_selection/_search.py:282: UserWarning: The total space of parameters 4 is smaller than n_iter=100. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  20 | elapsed:    0.3s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.78      0.63        68\n",
      "           1       0.35      0.15      0.21        54\n",
      "\n",
      "    accuracy                           0.50       122\n",
      "   macro avg       0.44      0.46      0.42       122\n",
      "weighted avg       0.45      0.50      0.45       122\n",
      "\n",
      "f1-micro : 0.5\n",
      "precision : 0.5\n",
      "recall : 0.5\n",
      "confusion_matrix:\n",
      " [[53 15]\n",
      " [46  8]]\n",
      "TN=53,FP=15,FN=46,TP=8\n",
      "-------------------------\n",
      "PoP : 42\n",
      "Avg news count within the pop: 2328.2565320665085\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/salim/.local/lib/python3.6/site-packages/sklearn/model_selection/_search.py:282: UserWarning: The total space of parameters 4 is smaller than n_iter=100. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  20 | elapsed:    0.3s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.78      0.65        67\n",
      "           1       0.42      0.21      0.28        53\n",
      "\n",
      "    accuracy                           0.53       120\n",
      "   macro avg       0.49      0.49      0.46       120\n",
      "weighted avg       0.50      0.53      0.48       120\n",
      "\n",
      "f1-micro : 0.525\n",
      "precision : 0.525\n",
      "recall : 0.525\n",
      "confusion_matrix:\n",
      " [[52 15]\n",
      " [42 11]]\n",
      "TN=52,FP=15,FN=42,TP=11\n",
      "-------------------------\n",
      "PoP : 47\n",
      "Avg news count within the pop: 2605.4299287410927\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/salim/.local/lib/python3.6/site-packages/sklearn/model_selection/_search.py:282: UserWarning: The total space of parameters 4 is smaller than n_iter=100. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  20 | elapsed:    0.3s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.61      0.59        67\n",
      "           1       0.45      0.40      0.42        52\n",
      "\n",
      "    accuracy                           0.52       119\n",
      "   macro avg       0.51      0.51      0.51       119\n",
      "weighted avg       0.52      0.52      0.52       119\n",
      "\n",
      "f1-micro : 0.5210084033613446\n",
      "precision : 0.5210084033613446\n",
      "recall : 0.5210084033613446\n",
      "confusion_matrix:\n",
      " [[41 26]\n",
      " [31 21]]\n",
      "TN=41,FP=26,FN=31,TP=21\n",
      "-------------------------\n",
      "PoP : 52\n",
      "Avg news count within the pop: 2882.603325415677\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/salim/.local/lib/python3.6/site-packages/sklearn/model_selection/_search.py:282: UserWarning: The total space of parameters 4 is smaller than n_iter=100. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  20 | elapsed:    0.3s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.80      0.69        66\n",
      "           1       0.55      0.31      0.40        51\n",
      "\n",
      "    accuracy                           0.59       117\n",
      "   macro avg       0.58      0.56      0.54       117\n",
      "weighted avg       0.58      0.59      0.56       117\n",
      "\n",
      "f1-micro : 0.5897435897435898\n",
      "precision : 0.5897435897435898\n",
      "recall : 0.5897435897435898\n",
      "confusion_matrix:\n",
      " [[53 13]\n",
      " [35 16]]\n",
      "TN=53,FP=13,FN=35,TP=16\n",
      "-------------------------\n",
      "PoP : 57\n",
      "Avg news count within the pop: 3159.776722090261\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/salim/.local/lib/python3.6/site-packages/sklearn/model_selection/_search.py:282: UserWarning: The total space of parameters 4 is smaller than n_iter=100. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  20 | elapsed:    0.3s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.65      0.59        66\n",
      "           1       0.36      0.26      0.30        50\n",
      "\n",
      "    accuracy                           0.48       116\n",
      "   macro avg       0.45      0.46      0.45       116\n",
      "weighted avg       0.46      0.48      0.47       116\n",
      "\n",
      "f1-micro : 0.4827586206896552\n",
      "precision : 0.4827586206896552\n",
      "recall : 0.4827586206896552\n",
      "confusion_matrix:\n",
      " [[43 23]\n",
      " [37 13]]\n",
      "TN=43,FP=23,FN=37,TP=13\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for pop in [2,7,12,17,22,27,32,37,42,47,52,57]:\n",
    "    #text = load_text_data(\"/home/salim/Coding/Masters Project/Dataset/stocktwits_by_company_name/AAL_tweet_score.csv\",'text')\n",
    "    text = load_text_data(\"/home/salim/Coding/Masters Project/Dataset/news_by_company/AAL .csv\",'messages')\n",
    "    price = load_price_data(\"/home/salim/Coding/Masters Project/Dataset/price_data_by_company/AAL.csv\") #Note that it's 0/1 now, not -1/0/1\n",
    "    frame = merge_frame(price,text,'time')\n",
    "    frame = frame.sort_values(by=['time'])\n",
    "    frame = frame.set_index('time')\n",
    "    frame = get_sentiment(frame)\n",
    "    frame = frame.tail(458)\n",
    "    l = []\n",
    "    for i in frame['text']:\n",
    "        if not isinstance(i,float):\n",
    "            l.append(i[0])\n",
    "        else:\n",
    "            l.append('')\n",
    "    frame['text'] = l\n",
    "    frame['rel_pol'] = window_sentiment(frame,pop)\n",
    "    frame['rel_pol'] = frame['rel_pol'].fillna(0.0)\n",
    "    frame = frame['2019-02-01':'2020-10-01']\n",
    "    print('PoP :',pop)\n",
    "    print('Avg news count within the pop:',avg_news(frame, pop))\n",
    "    #frame = frame.head(441)\n",
    "    #frame['rel_pol'] = frame['rel_pol']\n",
    "    frame['rel_pol'] = frame['rel_pol']  / frame['rel_pol'].abs().max()*4\n",
    "    #frame['close_days'] = frame['close'].shift(-1)\n",
    "    frame = change_sentiment(frame,-1)\n",
    "    frame = frame.dropna()\n",
    "    #print(frame['text'].tolist())\n",
    "    \n",
    "    #Adding the weighted event scores to the sentiment feature\n",
    "    # Uncomment this to use event-extraction\n",
    "    '''\n",
    "    cleaned_articles = cleaning(frame['text'])\n",
    "    event_vector = create_event_vector(cleaned_articles)\n",
    "    predicted_events = get_predicted_event(event_vector)\n",
    "    weighted_scores = get_weighted_scores(predicted_events)\n",
    "    frame['rel_pol'] = frame['rel_pol'] * weighted_scores\n",
    "    '''\n",
    "    frame = frame[['close','rel_pol','text','change']]\n",
    "    #print(frame)\n",
    "    \n",
    "    #classification_model(frame)\n",
    "    optimized_classifier(frame)\n",
    "    \n",
    "    \n",
    "    #print(frame['rel_pol'].tolist())\n",
    "    #correlation_raph(frame)\n",
    "    #print('RMSPE:',get_prediction(frame,False))\n",
    "    print('-'*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>rel_pol</th>\n",
       "      <th>text</th>\n",
       "      <th>change</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-03-12</th>\n",
       "      <td>184.18</td>\n",
       "      <td>0.794556</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-13</th>\n",
       "      <td>187.35</td>\n",
       "      <td>7.704619</td>\n",
       "      <td>top health care stockstop health care stocks j...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-14</th>\n",
       "      <td>187.76</td>\n",
       "      <td>1.167740</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-15</th>\n",
       "      <td>191.24</td>\n",
       "      <td>1.167740</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-18</th>\n",
       "      <td>191.31</td>\n",
       "      <td>9.445689</td>\n",
       "      <td>top health care stockstop health care stocks j...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-24</th>\n",
       "      <td>240.32</td>\n",
       "      <td>-0.320250</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-25</th>\n",
       "      <td>243.82</td>\n",
       "      <td>-0.320250</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-28</th>\n",
       "      <td>247.03</td>\n",
       "      <td>-0.156313</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-29</th>\n",
       "      <td>248.30</td>\n",
       "      <td>-0.156313</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30</th>\n",
       "      <td>254.16</td>\n",
       "      <td>-0.156313</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>394 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             close   rel_pol  \\\n",
       "time                           \n",
       "2019-03-12  184.18  0.794556   \n",
       "2019-03-13  187.35  7.704619   \n",
       "2019-03-14  187.76  1.167740   \n",
       "2019-03-15  191.24  1.167740   \n",
       "2019-03-18  191.31  9.445689   \n",
       "...            ...       ...   \n",
       "2020-09-24  240.32 -0.320250   \n",
       "2020-09-25  243.82 -0.320250   \n",
       "2020-09-28  247.03 -0.156313   \n",
       "2020-09-29  248.30 -0.156313   \n",
       "2020-09-30  254.16 -0.156313   \n",
       "\n",
       "                                                         text  change  \n",
       "time                                                                   \n",
       "2019-03-12                                                          1  \n",
       "2019-03-13  top health care stockstop health care stocks j...       1  \n",
       "2019-03-14                                                          1  \n",
       "2019-03-15                                                          1  \n",
       "2019-03-18  top health care stockstop health care stocks j...       1  \n",
       "...                                                       ...     ...  \n",
       "2020-09-24                                                          1  \n",
       "2020-09-25                                                          1  \n",
       "2020-09-28                                                          1  \n",
       "2020-09-29                                                          1  \n",
       "2020-09-30                                                          1  \n",
       "\n",
       "[394 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
